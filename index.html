<!-- This layout is used in all pages. Making changes here will efect all pages. We recommend not to change anything here. --> <!DOCTYPE html><html lang="en"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /><link rel="dns-prefetch" href="//fonts.googleapis.com" /><link rel="dns-prefetch" href="//google-analytics.com" /><link rel="dns-prefetch" href="//www.google-analytics.com" /><link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com" /><link rel="dns-prefetch" href="//ajax.googleapis.com" /><link rel="dns-prefetch" href="//fonts.gstatic.com" /><link rel="prerender" href="/contact/" /><link rel="prerender" href="/blog/" /><title>Welcome!</title><meta name="generator" content="Jekyll v3.8.4" /><meta property="og:title" content="Welcome!" /><meta property="og:locale" content="en_US" /><meta name="description" content="Personal website of Mayalen Etcheverry." /><meta property="og:description" content="Personal website of Mayalen Etcheverry." /><link rel="canonical" href="http://localhost:4000/" /><meta property="og:url" content="http://localhost:4000/" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Welcome!" /><script type="application/ld+json"> {"headline":"Welcome!","@type":"WebSite","url":"http://localhost:4000/","description":"Personal website of Mayalen Etcheverry.","@context":"https://schema.org"}</script><meta property="og:image" content="http://localhost:4000" /><link rel="stylesheet" href="/assets/css/main.css" /> <!-- Replace these icons with your own. --><link rel="apple-touch-icon" sizes="60x60" href="/assets/icon/favicon.svg" /><link rel="apple-touch-icon" sizes="114x114" href="/assets/icon/favicon.svg" /><link rel="apple-touch-icon" sizes="152x152" href="/assets/icon/favicon.svg" /><link rel="icon" type="image/png" sizes="192x192" href="/assets/icon/favicon.svg" /><link rel="shortcut icon" href="/assets/icon/favicon.svg" type="image/x-icon" /><link rel="icon" href="/assets/icon/favicon.svg" type="image/x-icon" /><script src="https://cdn.jsdelivr.net/npm/ga-lite@1/dist/ga-lite.min.js" async></script><script> var galite = galite || {}; galite.UA = 'UA-92266803-3';</script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script></head><body><div class="loader"><div class="lds-ring"><div></div><div></div><div></div><div></div></div></div><div class="wrapper"><div class="container-grid"><div class="sidebar"><div class="author-container shadow"><div class="author"><img src="/assets/images/author-image.jpg" width="100%" height="auto;" alt="" onclick="location.href='/'"></div><div class="about text-center"><h1 class="title"><a href="/"></a></h1></div><div class="bio text-center"> <p class="m1"><b>Mayalen Etcheverry</b></p> <p class="m0">Ph.D. in Machine Learning</p> <p class="m0"><i class="fa fa-map-pin fa"></i> INRIA Bordeaux, France</p> <p class="m0"><i class="fa fa-envelope fa"></i> etcheverry.mayalen@gmail.com</p></div><hr class="dashed"><div class="social text-center"> <ul class="portfolio p0"> </ul> <ul class="sm p0 m0a"> <li><a href="https://twitter.com/mayalen_etc"><i class="fa fa-twitter"></i></a></li> <li><a href="https://fr.linkedin.com/in/mayalenetcheverry"><i class="fa fa-linkedin"></i></a></li> <li><a href="https://github.com/mayalenE"><i class="fa fa-github"></i></a></li> <li><a href="https://scholar.google.com/citations?user=DjONosMAAAAJ&hl=no"><?xml version="1.0" encoding="UTF-8" standalone="no"?> <svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" aria-label="Google Scholar" role="img" viewBox="0 0 100 100" version="1.1" id="svg6" width="40" height="40"><metadata id="metadata12"> <rdf:RDF> <cc:Work rdf:about=""> <dc:format>image/svg+xml</dc:format> <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage" /> <dc:title></dc:title> </cc:Work> </rdf:RDF> </metadata> <defs id="defs10" /> <ellipse style="opacity:0.997;fill:#1abc9c;fill-opacity:1;stroke-width:1.16623" id="path837" cx="49.759441" cy="50.655872" ry="47.955448" rx="48" /> <path fill="#fff" d="M 43.68683,23.95016 25.884996,39.589144 h 11.47968 c 0.831874,7.486736 6.821273,10.647813 12.977036,11.146932 -1.164602,2.994714 -0.665483,4.492043 1.164602,6.488518 -7.154001,0.166309 -17.136324,4.325679 -17.136324,11.146953 0.665482,7.486724 10.481442,8.984084 15.306239,8.984084 6.322127,0.16631 13.476128,-3.161077 14.973485,-8.984084 0.665482,-5.823036 -1.663721,-8.984113 -5.157526,-11.812435 -3.826559,-2.994714 -4.658434,-4.658434 -3.493832,-6.654882 2.495595,-2.82835 5.823036,-4.49207 6.488518,-8.484994 0.332755,-2.828325 -0.332727,-4.65842 -0.998238,-7.154001 l 7.486756,-6.322143 -0.166308,2.661955 C 68.309965,30.937792 67.97721,31.60328 67.97721,32.102399 V 49.23872 c 0.332755,2.162839 3.660196,1.830111 3.82656,0 V 32.102399 c 0,-0.499119 -0.332727,-1.164607 -0.831846,-1.33098 v -4.159303 l 2.661958,-2.661956 z m 9.649568,23.458475 C 43.187711,49.072356 38.862032,32.934259 47.014271,30.937792 c 9.31684,-1.830093 13.808883,14.308004 6.322127,16.470843 z m -0.831847,12.145163 c 9.982323,2.162839 10.148687,10.481469 1.663721,12.977036 -7.320392,1.497357 -13.642519,-0.665483 -13.476155,-4.991161 0,-4.159316 5.823035,-7.985875 11.812434,-7.985875 z" id="path4" style="fill:#ffffff;fill-opacity:1;stroke-width:0.166372" /> </svg> </a></li> <li><a href="assets/pdf/Resume_Mayalen_Etcheverry.pdf"><svg id="cvsvg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="40" height="40" viewBox="0 0 100 100"> <circle id="backcircle" stroke="none" fill="#1abc9c" r="48%" cx="50%" cy="50%"></circle><g transform="translate(50 50) scale(0.69 0.69) rotate(0) translate(-50 -50)" style="fill:#ffffff"><svg fill="#ffffff" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" x="0px" y="0px" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500;" xml:space="preserve"><g><polygon points="404,120.9 311.9,120.9 311.9,30 "></polygon><path d="M404,130.9h-92.1c-5.5,0-10-4.5-10-10V30c0-4,2.4-7.7,6.1-9.2c3.7-1.6,8-0.7,10.9,2.1l92.1,90.9c2.9,2.8,3.8,7.2,2.2,10.9 C411.7,128.4,408.1,130.9,404,130.9z M321.9,110.9h57.7l-57.7-57V110.9z"></path></g><g><path d="M203.8,237.6h18.7c11,0,20-9,20-20s-9-20-20-20h-18.7c-35.4,0-64.2,28.8-64.2,64.2v29.5c0,35.4,28.8,64.2,64.2,64.2h18.7 c11,0,20-9,20-20s-9-20-20-20h-18.7c-13.4,0-24.2-10.9-24.2-24.2v-29.5C179.5,248.5,190.4,237.6,203.8,237.6z"></path><path d="M348.7,198.1c-10.8-2.3-21.4,4.6-23.7,15.4L307.2,296l-15.6-82.5c-2.1-10.9-12.5-18-23.4-15.9c-10.9,2.1-18,12.5-15.9,23.4 l19.7,104.6c3.2,16.8,17.3,28.7,34.4,28.9c0.2,0,0.3,0,0.5,0c16.9,0,31.1-11.5,34.7-28l22.4-104.4 C366.4,211,359.5,200.4,348.7,198.1z"></path></g><path d="M417.9,105.7l-55-53.4l-37-36.5c-3.9-3.9-9.2-5.9-14.5-5.8c-0.1,0-0.1,0-0.2,0H96c-11,0-20,9-20,20v443c0,11,9,20,20,20h308 c11,0,20-9,20-20V120C424,114.6,421.8,109.4,417.9,105.7z M355.3,100.9h-23.3v-23l3,3L355.3,100.9z M281,453 c-91.1,0-165-73.9-165-165V50h175.9v70.9c0,11,9,20,20,20H384V453H281z"></path></svg></g></svg> </a></li> </ul></div></div></div><style type="text/css"> #cvsvg:hover #backcircle {fill:#444;} </style><div class="main"><div class="main-container shadow"><div class="title-space"> <h1>Welcome!</h1> <!--<div class="input-group mb-3" data-aos="zoom-in"> <input type="text" class="form-control" id="search-input"><div class="input-group-append"> <span class="input-group-text"><i class="fa fa-search"></i></span></div></div>--></div><hr class="dashed"> <main> <p> I'm interested in many things related to <b>Machine Learning</b>, <b>AI for Science</b>, and <b>Open-Endedness</b>.<br> </p> <p> I recently obtained my Ph.D. in Machine Learning under the supervision of <a href="http://pyoudeyer.com/">Pierre-Yves Oudeyer</a> and <a href="https://clement-moulin-frier.github.io/">Clément Moulin-Frier</a> at the <a href="https://flowers.inria.fr/">FLOWERS</a> team at Inria, Bordeaux; and in collaboration with the <a href="https://poietis.com/">Poietis</a> biotech company. Last year, I also went on a 4-month visit to Dr. <a href="https://drmichaellevin.org/people/">Michael Levin</a> and his team at the <a href="https://drmichaellevin.org/">Levin Lab</a> at Tufts University. </p> <p> Previously, I have been at <a href="https://www.ucl.ac.uk/">University College of London</a> where I completed my Master of Science (MSc) in computer vision and at <a href="https://www.telecom-paris.fr/">Télécom Paristech</a> where I did my Master of Engineering (MEng). I also spent a year as an AI research intern at <a href="https://www.siemens-healthineers.com/">Siemens Healthineers</a>, Princeton N.J., working on deep learning and reinforcement learning algorithms for healthcare. My extended CV can be found <a href="assets/pdf/Resume_Mayalen_Etcheverry.pdf">here</a>. </p> <h2 class="m0a" data-aos="fade-up">News:</h2><div> <b>🚨 Now exploring opportunities for what comes next, please get in touch! 🚨</b> <ul> <li> <b>04/2024:</b> New <a href="https://mayalene.github.io/simple-foc-assistant">simple-foc-assistant</a> tutorial to learn how to build a SimpleFOC AI Assistant with RAG </li> <li> <b>03/2024:</b> New <a href="https://mayalene.github.io/sketch-transformer">sketch-transformer</a> tutorial to train transformers to generate human-like sketches </li> <li> <b>02/2024:</b> Our paper about <a href="https://elifesciences.org/reviewed-preprints/92683">AI-driven discovery of GRN behaviors</a> got accepted into the eLife journal </li> <li> <b>12/2023:</b> Co-organized the <a href="https://sites.google.com/view/aloe2023">Agent Learning in Open-Endedness Workshop</a> held at NeurIPS 2023 🌱 </li> <li> <b>11/2023:</b> Defended my thesis <i>"Curiosity-driven AI for Science: Automated Discovery of Self-Organized Structures"</i>. Thank you to my amazing jury Alan Aspuru-Guzik, Sebastian Risi, Melanie Mitchell, Jeff Clune, and Nicolas Brodu; and supervisors Pierre-Yves Oudeyer, Clément Moulin-Frier and Marc Nicodème 🙏 </li> <li> <b>09/2023:</b> New <a href="https://developmentalsystems.org/curious-exploration-of-grn-competencies/tuto1.html">tutorial serie </a> on how to use diversity search to explore behaviors of biological networks </li> <li> <b>07/2023:</b> Our <a href="https://direct.mit.edu/isal/proceedings/isal/35/131/116921">Flow Lenia</a> paper won the <a href="https://2023.alife.org/awards/">best paper award</a> at ALife 2023 conference 👾 </li> <li> <b>07/2023:</b> Released the <a href="">SBMLtoODEjax</a> package, check out the <a href="https://github.com/flowersteam/sbmltoodejax">documentation and tutorials</a> for more info </li> <!-- <li> <b>08-12/2022:</b> Was very lucky to visit Dr. <a href="https://drmichaellevin.org/">Michael Levin</a> and his team at Tufts University </li> <li> <b>04/2022:</b> Participated to the <a href="https://sites.google.com/view/collective-learning">From Cells to Societies</a> workshop, with <a href="https://iclr.cc/virtual/2022/workshop/4553"> [invited talk + dicussions]</a> </li> <li> <b>01/2022:</b> New <a href="https://developmentalsystems.org/sensorimotor-lenia">blogpost</a> where we study the emergence of sensorimotor agency within a CA environment </li> <li> <b>07/2021:</b> New <a href="https://mayalene.github.io/evocraftsearch/">blogpost</a> about our participation to the <a href="https://evocraft.life/">MineCraft Open-Endedness Challenge</a> (Runner-up) </li> <li> <b>12/2020:</b> NeurIPS 2020 <a href="https://slideslive.com/38938556/hierarchicallyorganized-latent-modules-for-exploratory-search-in-morphogenetic-systems">talk</a> about our paper on meta-diversity search to explore self-organizing systems </li> <li> <b>04/2020:</b> New <a href="http://developmentalsystems.org/intrinsically_motivated_discovery_of_diverse_patterns">blogpost</a> about our paper on intrinsically-motivated discovery in self-organizing systems </li> --> </ul></div><h2 class="m0a">Thesis:</h2><div class="projects"> <a href="/thesis/2023-11-20-manuscript/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/publications/thesis/manuscript_thumbnail.png)" ></div><p class="title text-center">Manuscript</p></div></a> <a href="/thesis/2023-11-20-slides/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/publications/thesis/slides_thumbnail.png)" ></div><p class="title text-center">Defense Slides</p></div></a> <a href="/thesis/2023-11-20-video/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/publications/thesis/video_thumbnail.png)" ></div><p class="title text-center">Defense Video</p></div></a></div><h2 class="m0a" data-aos="fade-up">Publications:</h2><div class="publication" data-aos="fade-up" style="margin:0em"> <i><b>*</b> stands for equal contribution</i><div class="publication-grid"> <ol class="bibliography"><li><a href="https://developmentalsystems.org/curious-exploration-of-grn-competencies/" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/etcheverry2023aidriven/paper_fig_1.png"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">AI-driven Automated Discovery Tools Reveal Diverse Behavioral Competencies of Biological Networks</p> <p class="authors"> <b>Mayalen Etcheverry</b>, Clément Moulin-Frier, Pierre-Yves Oudeyer, Michael Levin </p> <p class="venue"> <b>eLife Journal (2024)</b> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexetcheverry2023aidriven()">abstract</a> | <a href="https://developmentalsystems.org/curious-exploration-of-grn-competencies/">webpage</a> | <a href="https://osf.io/s6thq/">pdf</a> | <a href="https://elifesciences.org/reviewed-preprints/92683">publication</a> | <a href="https://github.com/flowersteam/curious-exploration-of-grn-competencies">code</a> | <a href="https://github.com/flowersteam/curious-exploration-of-grn-competencies/tree/main/notebooks">tutorials</a> |</div></div></a><div id="aetcheverry2023aidriven" style="display: none;"> <p class="abstract">Many applications in biomedicine and synthetic bioengineering depend on the ability to understand, map, predict, and control the complex, context-sensitive behavior of chemical and genetic networks. The emerging field of diverse intelligence has offered frameworks with which to investigate and exploit surprising problem-solving capacities of unconventional agents. However, for systems that are not conventional animals used in behavior science, there are few quantitative tools that facilitate exploration of their competencies, especially when their complexity makes it infeasible to use unguided exploration . Here, we formalize and investigate a view of gene regulatory networks as agents navigating a problem space. We develop automated tools to efficiently map the repertoire of robust goal states that GRNs can reach despite perturbations. These tools rely on two main contributions that we make in this paper: (1) Using curiosity-driven exploration algorithms, originating from the AI community to explore the range of behavioral abilities of a given system, that we adapt and leverage to automatically discover the range of reachable goal states of GRNs and (2) Proposing a battery of empirical tests inspired by implementation-agnostic behaviorist approaches to assess their navigation competencies. Our data reveal that models inferred from real biological data can reach a surprisingly wide spectrum of steady states, while showcasing various competencies that living agents often exhibit, in physiological network dynamics and that do not require structural changes of network properties or connectivity. Furthermore, we investigate the applicability of the discovered “behavioral catalogs” for comparing the evolved competencies across classes of evolved biological networks, as well as for the design of drug interventions in biomedical contexts or for the design of synthetic gene networks in bioengineering. Altogether, these automated tools and the resulting emphasis on behavior-shaping and exploitation of innate competencies open the path to better interrogation platforms for exploring the complex behavior of biological networks in an efficient and cost-effective manner. To read the interactive version of this paper, please visit https://developmentalsystems.org/curious-exploration-of-grn-competencies.</p></div><script> function toggleBibtexetcheverry2023aidriven(parameter) { var x= document.getElementById('aetcheverry2023aidriven'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://developmentalsystems.org/sbmltoodejax/" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/etcheverry2023sbmltoodejax/biomd_simulation.svg"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">SBMLtoODEjax: Efficient Simulation and Optimization of Biological Network Models in JAX</p> <p class="authors"> <b>Mayalen Etcheverry</b>, Michael Levin, Clément Moulin-Frier, Pierre-Yves Oudeyer </p> <p class="venue"> <i>AI for Science</i> Workshop at <b>NeurIPS 2023 (Poster)</b> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexetcheverry2023sbmltoodejax()">abstract</a> | <a href="https://developmentalsystems.org/sbmltoodejax/">webpage</a> | <a href="https://arxiv.org/pdf/2307.08452.pdf">pdf</a> | <a href="https://openreview.net/forum?id=exP6UntwqJ">publication</a> | <a href="https://github.com/flowersteam/sbmltoodejax">code</a> | <a href="https://developmentalsystems.org/sbmltoodejax/">documentation</a> | <a href="https://github.com/flowersteam/sbmltoodejax/tree/main/docs/source/tutorials/">tutorials</a> |</div></div></a><div id="aetcheverry2023sbmltoodejax" style="display: none;"> <p class="abstract">Developing methods to explore, predict and control the dynamic behavior of biological systems, from protein pathways to complex cellular processes, is an essential frontier of research for bioengineering and biomedicine. Thus, significant effort has gone in computational inference and mathematical modeling of biological systems. This effort has resulted in the development of large collections of publicly-available models, typically stored and exchanged on online platforms (such as the BioModels Database) using the Systems Biology Markup Language (SBML), a standard format for representing mathematical models of biological systems. SBMLtoODEjax is a lightweight library that allows to automatically parse and convert SBML models into python models written end-to-end in JAX, a high-performance numerical computing library with automatic differentiation capabilities. SBMLtoODEjax is targeted at researchers that aim to incorporate SBML-specified ordinary differential equation (ODE) models into their python projects and machine learning pipelines, in order to perform efficient numerical simulation and optimization with only a few lines of code. SBMLtoODEjax is available at https://github.com/flowersteam/sbmltoodejax.</p></div><script> function toggleBibtexetcheverry2023sbmltoodejax(parameter) { var x= document.getElementById('aetcheverry2023sbmltoodejax'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://developmentalsystems.org/sensorimotor-lenia-companion/" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/hamon2022learning/demo.gif"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Discovering Sensorimotor Agency in Cellular Automata using Diversity Search</p> <p class="authors"> Gautier Hamon*, <b>Mayalen Etcheverry*</b>, Bert Wang-Chak Chan, Clément Moulin-Frier, Pierre-Yves Oudeyer </p> <p class="venue"> Preprint </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexhamon2024learning()">abstract</a> | <a href="https://developmentalsystems.org/sensorimotor-lenia-companion/">webpage</a> | <a href="https://arxiv.org/pdf/2402.10236.pdf">pdf</a> | <a href="https://github.com/flowersteam/sensorimotor-lenia-search">code</a> | <a href="https://colab.research.google.com/drive/11mYwphZ8I4aur8KuHRR1HEg6ST5TI0RW?usp=sharing">tutorials</a> |</div></div></a><div id="ahamon2024learning" style="display: none;"> <p class="abstract">The research field of Artificial Life studies how life-like phenomena such as autopoiesis, agency, or self-regulation can self-organize in computer simulations. In cellular automata (CA), a key open-question has been whether it it is possible to find environment rules that self-organize robust “individuals” from an initial state with no prior existence of things like “bodies”, “brain”, “perception” or “action”. In this paper, we leverage recent advances in machine learning, combining algorithms for diversity search, curriculum learning and gradient descent, to automate the search of such “individuals”, i.e. localized structures that move around with the ability to react in a coherent manner to external obstacles and maintain their integrity, hence primitive forms of sensorimotor agency. We show that this approach enables to find systematically environmental conditions in CA leading to self-organization of such basic forms of agency. Through multiple experiments, we show that the discovered agents have surprisingly robust capabilities to move, maintain their body integrity and navigate among various obstacles. They also show strong generalization abilities, with robustness to changes of scale, random updates or perturbations from the environment not seen during training. We discuss how this approach opens new perspectives in AI and synthetic bioengineering.</p></div><script> function toggleBibtexhamon2024learning(parameter) { var x= document.getElementById('ahamon2024learning'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://sites.google.com/view/flowlenia/videos" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/plantec2023flow/flow_lenia.gif"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Flow-Lenia: Towards open-ended evolution in cellular automata through mass conservation and parameter localization</p> <p class="authors"> Erwan Plantec, Gautier Hamon, <b>Mayalen Etcheverry</b>, Pierre-Yves Oudeyer, Clément Moulin-Frier, Bert Wang-Chak Chan </p> <p class="venue"> <b>ALIFE 2023</b> <a style="font-weight: bold; color: $brand-color">(Best Paper Award)</a> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexplantec2023flow()">abstract</a> | <a href="https://sites.google.com/view/flowlenia/videos">webpage</a> | <a href="https://arxiv.org/pdf/2212.07906.pdf">pdf</a> | <a href="https://direct.mit.edu/isal/proceedings/isal/35/131/116921">publication</a> | <a href="https://www.youtube.com/watch?v=605DcOMwFLM">oral talk</a> | <a href="https://colab.research.google.com/drive/1l-Og8xRlc5ew0489swuud0Me7Sc5bCss?usp=sharing">code</a> |</div></div></a><div id="aplantec2023flow" style="display: none;"> <p class="abstract">The design of complex self-organising systems producing life-like phenomena, such as the open-ended evolution of virtual creatures, is one of the main goals of artificial life. Lenia, a family of cellular automata (CA) generalizing Conway’s Game of Life to continuous space, time and states, has attracted a lot of attention because of the wide diversity of self-organizing patterns it can generate. Among those, some spatially localized patterns (SLPs) resemble life-like artificial creatures and display complex behaviors. However, those creatures are found in only a small subspace of the Lenia parameter space and are not trivial to discover, necessitating advanced search algorithms. Furthermore, each of these creatures exist only in worlds governed by specific update rules and thus cannot interact in the same one. This paper proposes as mass-conservative extension of Lenia, called Flow Lenia, that solve both of these issues. We present experiments demonstrating its effectiveness in generating SLPs with complex behaviors and show that the update rule parameters can be optimized to generate SLPs showing behaviors of interest. Finally, we show that Flow Lenia enables the integration of the parameters of the CA update rules within the CA dynamics, making them dynamic and localized, allowing for multi-species simulations, with locally coherent update rules that define properties of the emerging creatures, and that can be mixed with neighbouring rules. We argue that this paves the way for the intrinsic evolution of selforganized artificial life forms within continuous CAs.</p></div><script> function toggleBibtexplantec2023flow(parameter) { var x= document.getElementById('aplantec2023flow'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://mayalenE.github.io/holmes/" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/etcheverry2020hierarchically/image.png"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Hierarchically Organized Latent Modules for Exploratory Search in Morphogenetic Systems</p> <p class="authors"> <b>Mayalen Etcheverry</b>, Clément Moulin-Frier, Pierre-Yves Oudeyer </p> <p class="venue"> <b>NeurIPS 2020</b> <a style="font-weight: bold; color: $brand-color">(Oral presentation, top 1%)</a> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexetcheverry2020hierarchically()">abstract</a> | <a href="https://mayalenE.github.io/holmes/">webpage</a> | <a href="https://proceedings.neurips.cc/paper/2020/file/33a5435d4f945aa6154b31a73bab3b73-Paper.pdf">pdf</a> | <a href="https://proceedings.neurips.cc/paper/2020/hash/33a5435d4f945aa6154b31a73bab3b73-Abstract.html">publication</a> | <a href="/assets/publications/etcheverry2020hierarchically/poster.pdf">poster</a> | <a href="https://slideslive.com/38938556/hierarchicallyorganized-latent-modules-for-exploratory-search-in-morphogenetic-systems">oral talk</a> | <a href="https://github.com/flowersteam/holmes">code</a> |</div></div></a><div id="aetcheverry2020hierarchically" style="display: none;"> <p class="abstract">Self-organization of complex morphological patterns from local interactions is a fascinating phenomenon in many natural and artificial systems. In the artificial world, typical examples of such morphogenetic systems are cellular automata. Yet, their mechanisms are often very hard to grasp and so far scientific discoveries of novel patterns have primarily been relying on manual tuning and ad hoc exploratory search. The problem of automated <i>diversity-driven</i> discovery in these systems was recently introduced [26,62], highlighting that two key ingredients are autonomous exploration and unsupervised representation learning to describe <q>relevant</q> degrees of variations in the patterns. In this paper, we motivate the need for what we call <i>Meta-diversity</i> search, arguing that there is not a unique ground truth <i>interesting</i> diversity as it strongly depends on the final observer and its motives. Using a continuous game-of-life system for experiments, we provide empirical evidences that relying on monolithic architectures for the behavioral embedding design tends to bias the final discoveries (both for hand-defined and unsupervisedly-learned features) which are unlikely to be aligned with the interest of a final end-user. To address these issues, we introduce a novel <i>dynamic</i> and <i>modular</i> architecture that enables unsupervised learning of a hierarchy of diverse representations. Combined with intrinsically motivated goal exploration algorithms, we show that this system forms a discovery assistant that can efficiently adapt its diversity search towards preferences of a user using only a very small amount of user feedback.</p></div><script> function toggleBibtexetcheverry2020hierarchically(parameter) { var x= document.getElementById('aetcheverry2020hierarchically'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://iclr.cc/virtual_2020/workshops_12.html" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/etcheverry2020progressive/holmes.png"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Progressive Growing of Self-Organized Hierarchical Representations for Exploration</p> <p class="authors"> <b>Mayalen Etcheverry</b>, Pierre-Yves Oudeyer, Chris Reinke </p> <p class="venue"> <i>Be-TR RL</i> Workshop at <b>ICLR 2020 (Poster)</b> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexetcheverry2020progressive()">abstract</a> | <a href="https://arxiv.org/pdf/2005.06369.pdf">pdf</a> | <a href="https://iclr.cc/virtual_2020/workshops_12.html">publication</a> | <a href="https://iclr.cc/virtual_2020/workshops_12.html">oral talk</a> |</div></div></a><div id="aetcheverry2020progressive" style="display: none;"> <p class="abstract">Designing agent that can autonomously discover and learn a diversity of structures and skills in unknown changing environments is key for lifelong machine learning. A central challenge is how to learn incrementally representations in order to progressively build a map of the discovered structures and re-use it to further explore. To address this challenge, we identify and target several key functionalities. First, we aim to build lasting representations and avoid catastrophic forgetting throughout the exploration process. Secondly we aim to learn a diversity of representations allowing to discover a “diversity of diversity” of structures (and associated skills) in complex high-dimensional environments. Thirdly, we target representations that can structure the agent discoveries in a coarse-to-fine manner. Finally, we target the reuse of such representations to drive exploration toward an “interesting” type of diversity, for instance leveraging human guidance. Current approaches in state representation learning rely generally on monolithic architectures which do not enable all these functionalities. Therefore, we present a novel technique to progressively construct a Hierarchy of Observation Latent Models for Exploration Stratification, called HOLMES. This technique couples the use of a dynamic modular model architecture for representation learning with intrinsically-motivated goal exploration processes (IMGEPs). The paper shows results in the domain of automated discovery of diverse self-organized patterns, considering as testbed the experimental framework from Reinke et al. (2019).</p></div><script> function toggleBibtexetcheverry2020progressive(parameter) { var x= document.getElementById('aetcheverry2020progressive'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://automated-discovery.github.io/" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/reinke2019intrinsically/image.png"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Intrinsically Motivated Exploration for Automated Discovery of Patterns in Morphogenetic Systems</p> <p class="authors"> Chris Reinke*, <b>Mayalen Etcheverry*</b>, Pierre-Yves Oudeyer </p> <p class="venue"> <b>ICLR 2020</b> <a style="font-weight: bold; color: $brand-color">(Oral presentation, top 2%)</a> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexreinke2019intrinsically()">abstract</a> | <a href="https://automated-discovery.github.io/">webpage</a> | <a href="https://arxiv.org/pdf/1908.06663.pdf">pdf</a> | <a href="https://openreview.net/forum?id=rkg6sJHYDr">publication</a> | <a href="https://iclr.cc/virtual_2020/poster_rkg6sJHYDr.html">oral talk</a> | <a href="https://developmentalsystems.org/intrinsically_motivated_discovery_of_diverse_patterns">blog</a> | <a href="https://github.com/flowersteam/automated_discovery_of_lenia_patterns">code</a> |</div></div></a><div id="areinke2019intrinsically" style="display: none;"> <p class="abstract">In many complex dynamical systems, artificial or natural, one can observe self-organization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL), have been widely used as abstract models enabling the study of various aspects of self-organization and morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized patterns in such models have so far relied on manual tuning of parameters and initial states,and on the human eye to identify “interesting” patterns. In this paper, we formulate the problem of automated discovery of diverse self-organized patterns in such high-dimensional complex dynamical systems, as well as a framework for experimentation and evaluation. Using a continuous GOL as a testbed, we show that recent intrinsically-motivated machine learning algorithms (POP-IMGEPs), initially developed for learning of inverse models in robotics, can be transposed and used in this novel application area. These algorithms combine intrinsically-motivated goal exploration and unsupervised learning of goal space representations. Goal space representations describe the “interesting” features of patterns for which diverse variations should be discovered. In particular, we compare various approaches to define and learn goal space representations from the perspective of discovering diverse spatially localized patterns. Moreover, we introduce an extension of a state-of-the-art POP-IMGEP algorithm which incrementally learns a goal representation using a deep auto-encoder, and the use of CPPN primitives for generating initialization parameters. We show that it is more efficient than several baselines and equally efficient as a system pre-trained on a hand-made database of patterns identified by human experts</p></div><script> function toggleBibtexreinke2019intrinsically(parameter) { var x= document.getElementById('areinke2019intrinsically'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-00889-5_29" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/etcheverry2018nonlinear/video.gif"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Nonlinear Adaptively Learned Optimization for Object Localization in 3D Medical Images</p> <p class="authors"> <b>Mayalen Etcheverry</b>, Bogdan Georgescu, Benjamin Odry, Thomas J Re, Kaushik Shivam, Geiger Bernhard, Nadar Mariappan, Sasa Grbic, Dorin Comaniciu </p> <p class="venue"> DLMIA workshop at <b>MICCAI 2018</b>, also abstract at <b>MED-NEURIPS 2018 (Poster)</b> </p> <p class="venue"> <b>US Patent</b>: <a style="font-size: 12px; font-style: italic;" href="https://patents.google.com/patent/US20190378291A1/en"> https://patents.google.com/patent/US20190378291A1/en </a> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexetcheverry2018nonlinear()">abstract</a> | <a href="/assets/publications/etcheverry2018nonlinear/paper.pdf">pdf</a> | <a href="https://link.springer.com/chapter/10.1007/978-3-030-00889-5_29">publication</a> | <a href="/assets/publications/etcheverry2018nonlinear/poster.pdf">poster</a> |</div></div></a><div id="aetcheverry2018nonlinear" style="display: none;"> <p class="abstract">Precise localization of anatomical structures in 3D medical images can support several tasks such as image registration, organ segmentation, lesion quantification and abnormality detection. This work proposes a novel method, based on deep reinforcement learning, to actively learn to localize an object in the volumetric scene. Given the parameterization of the sought object, an intelligent agent learns to optimize the parameters by performing a sequence of simple control actions. We show the applicability of our method by localizing boxes (9 degrees of freedom) on a set of acquired MRI scans of the brain region. We achieve high speed and high accuracy detection results, with robustness to challenging cases. This method can be applied to a broad range of problems and easily generalized to other type of imaging modalities.</p></div><script> function toggleBibtexetcheverry2018nonlinear(parameter) { var x= document.getElementById('aetcheverry2018nonlinear'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li></ol></div></div><h2 class="m0a">Blog posts / Tutorials:</h2><div class="projects"> <a href="/blog/simple-foc-assistant/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(https://github.com/mayalenE/simple-foc-assistant/blob/main/logo_white.png?raw=true)" ></div><p class="title text-center">Building a SimpleFOC AI Assistant with RAG</p></div></a> <a href="/blog/sketch-transformer/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(https://github.com/mayalenE/sketch-transformer/blob/main/sketching_cats.gif?raw=true)" ></div><p class="title text-center">Generate Human-like Sketches with Transformers</p></div></a> <a href="/blog/grn-tuto2/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/grn-tuto2.png)" ></div><p class="title text-center">Diversity Search to Explore Biological Networks (tuto 2)</p></div></a> <a href="/blog/grn-tuto1/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/grn-tuto1.png)" ></div><p class="title text-center">Diversity Search to Explore Biological Networks (tuto 1)</p></div></a> <a href="/blog/a-conversation/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/pattern6.gif)" ></div><p class="title text-center">A conversation with Nicholas Guttenberg about my work</p></div></a> <a href="/blog/sensorimotor-lenia/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/maze.gif)" ></div><p class="title text-center">Learning Sensorimotor Agency in Cellular Automata</p></div></a> <a href="/blog/evocraftsearch/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/evocraftsearch.png)" ></div><p class="title text-center">Meta-Diversity Search in Minecraft</p></div></a> <a href="/blog/imgep-gol/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/patterns_animals.png)" ></div><p class="title text-center">Automated discovery in a continuous Game of Life</p></div></a></div><h2 class="m0a">Open-source projects:</h2><div class="projects"> <a href="/projects/02-sbmltoodejax/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/sbmltoodejax.svg)" ></div><p class="title text-center">SBMLtoODEjax</p></div></a> <a href="/projects/01-autodisctool/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/adtool.gif)" ></div><p class="title text-center">Automated Discovery Tool</p></div></a></div></main><div id="search-container"></div><hr class="dashed"><footer><div class="text-right"> <p class="copy"> <i class="fa fa-envelope fa"></i> etcheverry.mayalen@gmail.com </p></div></footer></div></div></div></div><script src="//ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script><script> $(document).ready(function () { $(".loader").hide(); });</script><script> (function () { var css = document.createElement('link'); css.href = '//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css'; css.rel = 'stylesheet'; css.type = 'text/css'; document.getElementsByTagName('head')[0].appendChild(css); })();</script><noscript><link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> </noscript><script> $("#search-input").keyup(function () { $("main").hide(); $("search-container").show(); if (!$('#search-input').val()) { $("main").show(); $("search-container").hide(); } });</script><script src="/assets/js/jekyll-search.min.js" type="text/javascript"></script><script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-container'), searchResultTemplate: '<a class="nostyle" href="{url}"><div class="blog borders cards"><div class="image" style="background-image: url({image});"></div><div class="content"><h3 class="title">{title}</h3><p class="description">{description}</p></div></div></a>', noResultsText: 'No results found', json: '/search.json' })</script><script> jQuery(document).ready(function () { jQuery('.skillbar').each(function () { jQuery(this).find('.skillbar-bar').animate({ width: jQuery(this).attr('data-percent') }, 5000); }); });</script><script> (function () { var css = document.createElement('link'); css.href = 'https://unpkg.com/aos@2.3.1/dist/aos.css'; css.rel = 'stylesheet'; css.type = 'text/css'; document.getElementsByTagName('head')[0].appendChild(css); })();</script><noscript><link rel="stylesheet" href="https://unpkg.com/aos@2.3.1/dist/aos.css"> </noscript><script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script><script> AOS.init({ duration: 600, once: true, disable: 'mobile' });</script></body></html>