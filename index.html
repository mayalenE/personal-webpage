<!-- This layout is used in all pages. Making changes here will efect all pages. We recommend not to change anything here. --> <!DOCTYPE html><html lang="en"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /><link rel="dns-prefetch" href="//fonts.googleapis.com" /><link rel="dns-prefetch" href="//google-analytics.com" /><link rel="dns-prefetch" href="//www.google-analytics.com" /><link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com" /><link rel="dns-prefetch" href="//ajax.googleapis.com" /><link rel="dns-prefetch" href="//fonts.gstatic.com" /><link rel="prerender" href="/contact/" /><link rel="prerender" href="/blog/" /><title>Mayalen Etcheverry</title><meta name="generator" content="Jekyll v3.8.4" /><meta property="og:title" content="Mayalen Etcheverry" /><meta property="og:locale" content="en_US" /><meta name="description" content="Personal website of Mayalen Etcheverry." /><meta property="og:description" content="Personal website of Mayalen Etcheverry." /><link rel="canonical" href="http://localhost:4000/" /><meta property="og:url" content="http://localhost:4000/" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Mayalen Etcheverry" /><script type="application/ld+json"> {"headline":"Mayalen Etcheverry","@type":"WebSite","url":"http://localhost:4000/","description":"Personal website of Mayalen Etcheverry.","@context":"https://schema.org"}</script><meta property="og:image" content="http://localhost:4000" /><link rel="stylesheet" href="/assets/css/main.css" /> <!-- Replace these icons with your own. --><link rel="apple-touch-icon" sizes="60x60" href="/assets/icon/favicon.svg" /><link rel="apple-touch-icon" sizes="114x114" href="/assets/icon/favicon.svg" /><link rel="apple-touch-icon" sizes="152x152" href="/assets/icon/favicon.svg" /><link rel="icon" type="image/png" sizes="192x192" href="/assets/icon/favicon.svg" /><link rel="shortcut icon" href="/assets/icon/favicon.svg" type="image/x-icon" /><link rel="icon" href="/assets/icon/favicon.svg" type="image/x-icon" /><script src="https://cdn.jsdelivr.net/npm/ga-lite@1/dist/ga-lite.min.js" async></script><script> var galite = galite || {}; galite.UA = 'UA-92266803-3';</script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script></head><body><div class="loader"><div class="lds-ring"><div></div><div></div><div></div><div></div></div></div><div class="wrapper"><div class="container-grid"><div class="sidebar"><div class="author-container shadow"><div class="author"><img src="/assets/images/author-image.jpg" width="100%" height="auto;" alt="" onclick="location.href='/'"></div><div class="about text-center"><h1 class="title"><a href="/"></a></h1></div><div class="bio text-center"><p class="m0"><i class="fa fa-envelope fa"></i> mayalen.etcheverry@inria.fr</p></div><hr class="dashed"><div class="social text-center"> <ul class="portfolio p0"> </ul> <ul class="sm p0 m0a"> <li><a href="https://twitter.com/mayalen_etc"><i class="fa fa-twitter"></i></a></li> <li><a href="https://fr.linkedin.com/in/mayalenetcheverry"><i class="fa fa-linkedin"></i></a></li> <li><a href="https://github.com/mayalenE"><i class="fa fa-github"></i></a></li> <li><a href="https://scholar.google.com/citations?user=DjONosMAAAAJ&hl=no"><?xml version="1.0" encoding="UTF-8" standalone="no"?> <svg xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:cc="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" aria-label="Google Scholar" role="img" viewBox="0 0 100 100" version="1.1" id="svg6" width="40" height="40"><metadata id="metadata12"> <rdf:RDF> <cc:Work rdf:about=""> <dc:format>image/svg+xml</dc:format> <dc:type rdf:resource="http://purl.org/dc/dcmitype/StillImage" /> <dc:title></dc:title> </cc:Work> </rdf:RDF> </metadata> <defs id="defs10" /> <ellipse style="opacity:0.997;fill:#1abc9c;fill-opacity:1;stroke-width:1.16623" id="path837" cx="49.759441" cy="50.655872" ry="47.955448" rx="48" /> <path fill="#fff" d="M 43.68683,23.95016 25.884996,39.589144 h 11.47968 c 0.831874,7.486736 6.821273,10.647813 12.977036,11.146932 -1.164602,2.994714 -0.665483,4.492043 1.164602,6.488518 -7.154001,0.166309 -17.136324,4.325679 -17.136324,11.146953 0.665482,7.486724 10.481442,8.984084 15.306239,8.984084 6.322127,0.16631 13.476128,-3.161077 14.973485,-8.984084 0.665482,-5.823036 -1.663721,-8.984113 -5.157526,-11.812435 -3.826559,-2.994714 -4.658434,-4.658434 -3.493832,-6.654882 2.495595,-2.82835 5.823036,-4.49207 6.488518,-8.484994 0.332755,-2.828325 -0.332727,-4.65842 -0.998238,-7.154001 l 7.486756,-6.322143 -0.166308,2.661955 C 68.309965,30.937792 67.97721,31.60328 67.97721,32.102399 V 49.23872 c 0.332755,2.162839 3.660196,1.830111 3.82656,0 V 32.102399 c 0,-0.499119 -0.332727,-1.164607 -0.831846,-1.33098 v -4.159303 l 2.661958,-2.661956 z m 9.649568,23.458475 C 43.187711,49.072356 38.862032,32.934259 47.014271,30.937792 c 9.31684,-1.830093 13.808883,14.308004 6.322127,16.470843 z m -0.831847,12.145163 c 9.982323,2.162839 10.148687,10.481469 1.663721,12.977036 -7.320392,1.497357 -13.642519,-0.665483 -13.476155,-4.991161 0,-4.159316 5.823035,-7.985875 11.812434,-7.985875 z" id="path4" style="fill:#ffffff;fill-opacity:1;stroke-width:0.166372" /> </svg> </a></li> <li><a href="assets/pdf/Resume_Mayalen_Etcheverry.pdf"><svg id="cvsvg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="40" height="40" viewBox="0 0 100 100"> <circle id="backcircle" stroke="none" fill="#1abc9c" r="48%" cx="50%" cy="50%"></circle><g transform="translate(50 50) scale(0.69 0.69) rotate(0) translate(-50 -50)" style="fill:#ffffff"><svg fill="#ffffff" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" x="0px" y="0px" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500;" xml:space="preserve"><g><polygon points="404,120.9 311.9,120.9 311.9,30 "></polygon><path d="M404,130.9h-92.1c-5.5,0-10-4.5-10-10V30c0-4,2.4-7.7,6.1-9.2c3.7-1.6,8-0.7,10.9,2.1l92.1,90.9c2.9,2.8,3.8,7.2,2.2,10.9 C411.7,128.4,408.1,130.9,404,130.9z M321.9,110.9h57.7l-57.7-57V110.9z"></path></g><g><path d="M203.8,237.6h18.7c11,0,20-9,20-20s-9-20-20-20h-18.7c-35.4,0-64.2,28.8-64.2,64.2v29.5c0,35.4,28.8,64.2,64.2,64.2h18.7 c11,0,20-9,20-20s-9-20-20-20h-18.7c-13.4,0-24.2-10.9-24.2-24.2v-29.5C179.5,248.5,190.4,237.6,203.8,237.6z"></path><path d="M348.7,198.1c-10.8-2.3-21.4,4.6-23.7,15.4L307.2,296l-15.6-82.5c-2.1-10.9-12.5-18-23.4-15.9c-10.9,2.1-18,12.5-15.9,23.4 l19.7,104.6c3.2,16.8,17.3,28.7,34.4,28.9c0.2,0,0.3,0,0.5,0c16.9,0,31.1-11.5,34.7-28l22.4-104.4 C366.4,211,359.5,200.4,348.7,198.1z"></path></g><path d="M417.9,105.7l-55-53.4l-37-36.5c-3.9-3.9-9.2-5.9-14.5-5.8c-0.1,0-0.1,0-0.2,0H96c-11,0-20,9-20,20v443c0,11,9,20,20,20h308 c11,0,20-9,20-20V120C424,114.6,421.8,109.4,417.9,105.7z M355.3,100.9h-23.3v-23l3,3L355.3,100.9z M281,453 c-91.1,0-165-73.9-165-165V50h175.9v70.9c0,11,9,20,20,20H384V453H281z"></path></svg></g></svg> </a></li> </ul></div></div></div><style type="text/css"> #cvsvg:hover #backcircle {fill:#444;} </style><div class="main"><div class="main-container shadow"><div class="title-space"> <h1>Mayalen Etcheverry</h1><div class="input-group mb-3" data-aos="zoom-in"> <input type="text" class="form-control" id="search-input"><div class="input-group-append"> <span class="input-group-text"><i class="fa fa-search"></i></span></div></div></div><hr class="dashed"> <main> <p> I am a third year PhD student in Machine Learning under the supervision of <a href="http://pyoudeyer.com/">Pierre-Yves Oudeyer</a> and <a href="https://clement-moulin-frier.github.io/">Clément Moulin-Frier</a> at the <a href="https://flowers.inria.fr/">FLOWERS</a> team at Inria, Bordeaux; and in collaboration with the <a href="https://poietis.com/">Poietis</a> biotech company. This year, I also went on a 4-month visit to Dr. <a href="https://drmichaellevin.org/people/">Michael Levin</a> and his team at the <a href="https://drmichaellevin.org/">Levin Lab</a> at Tufts University. </p> <p> My research focuses on developing "curious" AI agents that can autonomously discover and learn a diversity of structures and skills in their environments. In particular, I am interested in applying those computational models to assist humans in mapping and navigating the space of possible outcomes of complex systems (artificial, physico-chemical and biological ones) and help solving challenging problems in science. </p> <p> Previously, I have been at <a href="https://www.ucl.ac.uk/">University College of London</a> where I completed my Master of Science (MSc) in computer vision and at <a href="https://www.telecom-paris.fr/">Télécom Paristech</a> where I did my Master of Engineering (MEng). I also spent a year as an AI research intern at <a href="https://www.siemens-healthineers.com/">Siemens Healthineers</a>, Princeton N.J., working on deep learning and reinforcement learning algorithms for healthcare. </p> <h2 class="m0a" data-aos="fade-up">News:</h2><div> <ul> <li> <b>08-12/2022:</b> Visited Dr. <a href="https://drmichaellevin.org/">Michael Levin</a> and his team at Tufts University </li> <li> <b>04/2022:</b> Participated to the <a href="https://sites.google.com/view/collective-learning">From Cells to Societies</a> workshop, with <a href="https://iclr.cc/virtual/2022/workshop/4553"> [invited talk + dicussions]</a> </li> <li> <b>01/2022:</b> New <a href="https://developmentalsystems.org/sensorimotor-lenia">blogpost</a> where we study the emergence of sensorimotor agency within a CA environment </li> <li> <b>07/2021:</b> New <a href="https://mayalene.github.io/evocraftsearch/">blogpost</a> about our participation to the <a href="https://evocraft.life/">MineCraft Open-Endedness Challenge</a> (Runner-up) </li> <li> <b>12/2020:</b> NeurIPS 2020 <a href="https://slideslive.com/38938556/hierarchicallyorganized-latent-modules-for-exploratory-search-in-morphogenetic-systems">talk</a> about our paper on meta-diversity search to explore self-organizing systems </li> <li> <b>04/2020:</b> New <a href="http://developmentalsystems.org/intrinsically_motivated_discovery_of_diverse_patterns">blogpost</a> about our paper on intrinsically-motivated discovery in self-organizing systems </li> </ul></div><h2 class="m0a" data-aos="fade-up">Publications:</h2><div class="publication" data-aos="fade-up"><div class="publication-grid"> <ol class="bibliography"><li><a href="" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/plantec2023flow/flow_lenia.gif"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Flow-Lenia: Towards open-ended evolution in cellular automata through mass conservation and parameter localization</p> <p class="authors"> Erwan Plantec, Gautier Hamon, <b>Mayalen Etcheverry</b>, Pierre-Yves Oudeyer, Clément Moulin-Frier, Bert Wang-Chak Chan </p> <p class="venue"> <b>ALIFE 2023</b> <a style="font-weight: bold; color: $brand-color">(Oral)</a> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexplantec2023flow()">abstract</a> | <a href="https://sites.google.com/view/flowlenia/videos">webpage</a> | <a href="https://arxiv.org/pdf/2212.07906.pdf">pdf</a> | <a href="https://colab.research.google.com/drive/1l-Og8xRlc5ew0489swuud0Me7Sc5bCss?usp=sharing">code</a> |</div></div></a><div id="aplantec2023flow" style="display: none;"> <p class="abstract">The design of complex self-organising systems producing life-like phenomena, such as the open-ended evolution of virtual creatures, is one of the main goals of artificial life. Lenia, a family of cellular automata (CA) generalizing Conway’s Game of Life to continuous space, time and states, has attracted a lot of attention because of the wide diversity of self-organizing patterns it can generate. Among those, some spatially localized patterns (SLPs) resemble life-like artificial creatures and display complex behaviors. However, those creatures are found in only a small subspace of the Lenia parameter space and are not trivial to discover, necessitating advanced search algorithms. Furthermore, each of these creatures exist only in worlds governed by specific update rules and thus cannot interact in the same one. This paper proposes as mass-conservative extension of Lenia, called Flow Lenia, that solve both of these issues. We present experiments demonstrating its effectiveness in generating SLPs with complex behaviors and show that the update rule parameters can be optimized to generate SLPs showing behaviors of interest. Finally, we show that Flow Lenia enables the integration of the parameters of the CA update rules within the CA dynamics, making them dynamic and localized, allowing for multi-species simulations, with locally coherent update rules that define properties of the emerging creatures, and that can be mixed with neighbouring rules. We argue that this paves the way for the intrinsic evolution of selforganized artificial life forms within continuous CAs.</p></div><script> function toggleBibtexplantec2023flow(parameter) { var x= document.getElementById('aplantec2023flow'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://mayalenE.github.io/holmes/" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/etcheverry2020hierarchically/image.png"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Hierarchically Organized Latent Modules for Exploratory Search in Morphogenetic Systems</p> <p class="authors"> <b>Mayalen Etcheverry</b>, Clément Moulin-Frier, Pierre-Yves Oudeyer </p> <p class="venue"> <b>NeurIPS 2020</b> <a style="font-weight: bold; color: $brand-color">(Oral)</a> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexetcheverry2020hierarchically()">abstract</a> | <a href="https://mayalenE.github.io/holmes/">webpage</a> | <a href="https://proceedings.neurips.cc/paper/2020/file/33a5435d4f945aa6154b31a73bab3b73-Paper.pdf">pdf</a> | <a href="https://proceedings.neurips.cc/paper/2020/hash/33a5435d4f945aa6154b31a73bab3b73-Abstract.html">publication</a> | <a href="/assets/publications/etcheverry2020hierarchically/poster.pdf">poster</a> | <a href="https://slideslive.com/38938556/hierarchicallyorganized-latent-modules-for-exploratory-search-in-morphogenetic-systems">oral talk</a> | <a href="https://github.com/flowersteam/holmes">code</a> |</div></div></a><div id="aetcheverry2020hierarchically" style="display: none;"> <p class="abstract">Self-organization of complex morphological patterns from local interactions is a fascinating phenomenon in many natural and artificial systems. In the artificial world, typical examples of such morphogenetic systems are cellular automata. Yet, their mechanisms are often very hard to grasp and so far scientific discoveries of novel patterns have primarily been relying on manual tuning and ad hoc exploratory search. The problem of automated <i>diversity-driven</i> discovery in these systems was recently introduced [26,62], highlighting that two key ingredients are autonomous exploration and unsupervised representation learning to describe <q>relevant</q> degrees of variations in the patterns. In this paper, we motivate the need for what we call <i>Meta-diversity</i> search, arguing that there is not a unique ground truth <i>interesting</i> diversity as it strongly depends on the final observer and its motives. Using a continuous game-of-life system for experiments, we provide empirical evidences that relying on monolithic architectures for the behavioral embedding design tends to bias the final discoveries (both for hand-defined and unsupervisedly-learned features) which are unlikely to be aligned with the interest of a final end-user. To address these issues, we introduce a novel <i>dynamic</i> and <i>modular</i> architecture that enables unsupervised learning of a hierarchy of diverse representations. Combined with intrinsically motivated goal exploration algorithms, we show that this system forms a discovery assistant that can efficiently adapt its diversity search towards preferences of a user using only a very small amount of user feedback.</p></div><script> function toggleBibtexetcheverry2020hierarchically(parameter) { var x= document.getElementById('aetcheverry2020hierarchically'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://iclr.cc/virtual_2020/workshops_12.html" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/etcheverry2020progressive/holmes.png"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Progressive Growing of Self-Organized Hierarchical Representations for Exploration</p> <p class="authors"> <b>Mayalen Etcheverry</b>, Pierre-Yves Oudeyer, Chris Reinke </p> <p class="venue"> <i>Beyond “tabula rasa” in reinforcement learning</i> workshop at <b>ICLR 2020</b> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexetcheverry2020progressive()">abstract</a> | <a href="https://arxiv.org/pdf/2005.06369.pdf">pdf</a> | <a href="https://iclr.cc/virtual_2020/workshops_12.html">publication</a> | <a href="https://iclr.cc/virtual_2020/workshops_12.html">oral talk</a> |</div></div></a><div id="aetcheverry2020progressive" style="display: none;"> <p class="abstract">Designing agent that can autonomously discover and learn a diversity of structures and skills in unknown changing environments is key for lifelong machine learning. A central challenge is how to learn incrementally representations in order to progressively build a map of the discovered structures and re-use it to further explore. To address this challenge, we identify and target several key functionalities. First, we aim to build lasting representations and avoid catastrophic forgetting throughout the exploration process. Secondly we aim to learn a diversity of representations allowing to discover a “diversity of diversity” of structures (and associated skills) in complex high-dimensional environments. Thirdly, we target representations that can structure the agent discoveries in a coarse-to-fine manner. Finally, we target the reuse of such representations to drive exploration toward an “interesting” type of diversity, for instance leveraging human guidance. Current approaches in state representation learning rely generally on monolithic architectures which do not enable all these functionalities. Therefore, we present a novel technique to progressively construct a Hierarchy of Observation Latent Models for Exploration Stratification, called HOLMES. This technique couples the use of a dynamic modular model architecture for representation learning with intrinsically-motivated goal exploration processes (IMGEPs). The paper shows results in the domain of automated discovery of diverse self-organized patterns, considering as testbed the experimental framework from Reinke et al. (2019).</p></div><script> function toggleBibtexetcheverry2020progressive(parameter) { var x= document.getElementById('aetcheverry2020progressive'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://automated-discovery.github.io/" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/reinke2019intrinsically/image.png"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Intrinsically Motivated Exploration for Automated Discovery of Patterns in Morphogenetic Systems</p> <p class="authors"> Chris Reinke*, <b>Mayalen Etcheverry*</b>, Pierre-Yves Oudeyer </p> <p class="venue"> <b>ICLR 2020</b> <a style="font-weight: bold; color: $brand-color">(Oral)</a> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexreinke2019intrinsically()">abstract</a> | <a href="https://automated-discovery.github.io/">webpage</a> | <a href="https://arxiv.org/pdf/1908.06663.pdf">pdf</a> | <a href="https://openreview.net/forum?id=rkg6sJHYDr">publication</a> | <a href="https://iclr.cc/virtual_2020/poster_rkg6sJHYDr.html">oral talk</a> | <a href="https://developmentalsystems.org/intrinsically_motivated_discovery_of_diverse_patterns">blog</a> | <a href="https://github.com/flowersteam/automated_discovery_of_lenia_patterns">code</a> |</div></div></a><div id="areinke2019intrinsically" style="display: none;"> <p class="abstract">In many complex dynamical systems, artificial or natural, one can observe self-organization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL), have been widely used as abstract models enabling the study of various aspects of self-organization and morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized patterns in such models have so far relied on manual tuning of parameters and initial states,and on the human eye to identify “interesting” patterns. In this paper, we formulate the problem of automated discovery of diverse self-organized patterns in such high-dimensional complex dynamical systems, as well as a framework for experimentation and evaluation. Using a continuous GOL as a testbed, we show that recent intrinsically-motivated machine learning algorithms (POP-IMGEPs), initially developed for learning of inverse models in robotics, can be transposed and used in this novel application area. These algorithms combine intrinsically-motivated goal exploration and unsupervised learning of goal space representations. Goal space representations describe the “interesting” features of patterns for which diverse variations should be discovered. In particular, we compare various approaches to define and learn goal space representations from the perspective of discovering diverse spatially localized patterns. Moreover, we introduce an extension of a state-of-the-art POP-IMGEP algorithm which incrementally learns a goal representation using a deep auto-encoder, and the use of CPPN primitives for generating initialization parameters. We show that it is more efficient than several baselines and equally efficient as a system pre-trained on a hand-made database of patterns identified by human experts</p></div><script> function toggleBibtexreinke2019intrinsically(parameter) { var x= document.getElementById('areinke2019intrinsically'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li> <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-00889-5_29" class="nostyle"><div class="container project borders cards" data-aos="fade-up" style="display: inline-block; margin: 4px; width: 100%;"><div class="col-xl-3 col-lg-5 col-md-6 col-sm-12 image"> <img src="/assets/publications/etcheverry2018nonlinear/video.gif"></div><div class="col-xl-9 col-lg-7 col-md-6 col-sm-12 text"> <p class="title">Nonlinear Adaptively Learned Optimization for Object Localization in 3D Medical Images</p> <p class="authors"> <b>Mayalen Etcheverry</b>, Bogdan Georgescu, Benjamin Odry, Thomas J Re, Kaushik Shivam, Geiger Bernhard, Nadar Mariappan, Sasa Grbic, Dorin Comaniciu </p> <p class="venue"> DLMIA workshop at <b>MICCAI 2018</b>, also abstract at <b>MED-NEURIPS 2018</b> </p> <p class="venue"> <b>US Patent</b>: <a style="font-size: 12px; font-style: italic;" href="https://patents.google.com/patent/US20190378291A1/en"> https://patents.google.com/patent/US20190378291A1/en </a> </p> | <a shape="rect" href="javascript:;" onclick="toggleBibtexetcheverry2018nonlinear()">abstract</a> | <a href="/assets/publications/etcheverry2018nonlinear/paper.pdf">pdf</a> | <a href="https://link.springer.com/chapter/10.1007/978-3-030-00889-5_29">publication</a> | <a href="/assets/publications/etcheverry2018nonlinear/poster.pdf">poster</a> |</div></div></a><div id="aetcheverry2018nonlinear" style="display: none;"> <p class="abstract">Precise localization of anatomical structures in 3D medical images can support several tasks such as image registration, organ segmentation, lesion quantification and abnormality detection. This work proposes a novel method, based on deep reinforcement learning, to actively learn to localize an object in the volumetric scene. Given the parameterization of the sought object, an intelligent agent learns to optimize the parameters by performing a sequence of simple control actions. We show the applicability of our method by localizing boxes (9 degrees of freedom) on a set of acquired MRI scans of the brain region. We achieve high speed and high accuracy detection results, with robustness to challenging cases. This method can be applied to a broad range of problems and easily generalized to other type of imaging modalities.</p></div><script> function toggleBibtexetcheverry2018nonlinear(parameter) { var x= document.getElementById('aetcheverry2018nonlinear'); if (x.style.display === 'none') { x.style.display = 'block'; } else { x.style.display = 'none'; } }</script></li></ol></div></div><h2 class="m0a">Blog posts:</h2><div class="projects"> <a href="/blog/a-conversation/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/pattern6.gif)" ></div><p class="title text-center">A conversation with Nicholas Guttenberg about my work</p></div></a> <a href="/blog/sensorimotor-lenia/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/maze.gif)" ></div><p class="title text-center">Learning Sensorimotor Agency in Cellular Automata</p></div></a> <a href="/blog/evocraftsearch/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/evocraftsearch.png)" ></div><p class="title text-center">Meta-Diversity Search in Minecraft</p></div></a> <a href="/blog/imgep-gol/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/patterns_animals.png)" ></div><p class="title text-center">Automated discovery in a continuous Game of Life</p></div></a></div><h2 class="m0a" data-aos="fade-up">Open-source projects:</h2><div class="projects"> <a href="/projects/2023-01-01-autodisctool/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/adtool.gif)"></div><p class="title text-center">Automated Discovery Tool</p></div></a> <a href="/projects/2023-03-01-sbmltoodejax/" class="nostyle p-3"><div class="project borders cards" data-aos="fade-up"><div class="project-image" style="background-image: url(/assets/images/sbmltoodejax.png)"></div><p class="title text-center">SBMLtoODEjax</p></div></a></div></main><div id="search-container"></div><hr class="dashed"><footer><div class="text-right"> <p class="copy"> <i class="fa fa-envelope fa"></i> mayalen.etcheverry@inria.fr </p></div></footer></div></div></div></div><script src="//ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script><script> $(document).ready(function () { $(".loader").hide(); });</script><script> (function () { var css = document.createElement('link'); css.href = '//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css'; css.rel = 'stylesheet'; css.type = 'text/css'; document.getElementsByTagName('head')[0].appendChild(css); })();</script><noscript><link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> </noscript><script> $("#search-input").keyup(function () { $("main").hide(); $("search-container").show(); if (!$('#search-input').val()) { $("main").show(); $("search-container").hide(); } });</script><script src="/assets/js/jekyll-search.min.js" type="text/javascript"></script><script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-container'), searchResultTemplate: '<a class="nostyle" href="{url}"><div class="blog borders cards"><div class="image" style="background-image: url({image});"></div><div class="content"><h3 class="title">{title}</h3><p class="description">{description}</p></div></div></a>', noResultsText: 'No results found', json: '/search.json' })</script><script> jQuery(document).ready(function () { jQuery('.skillbar').each(function () { jQuery(this).find('.skillbar-bar').animate({ width: jQuery(this).attr('data-percent') }, 5000); }); });</script><script> (function () { var css = document.createElement('link'); css.href = 'https://unpkg.com/aos@2.3.1/dist/aos.css'; css.rel = 'stylesheet'; css.type = 'text/css'; document.getElementsByTagName('head')[0].appendChild(css); })();</script><noscript><link rel="stylesheet" href="https://unpkg.com/aos@2.3.1/dist/aos.css"> </noscript><script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script><script> AOS.init({ duration: 600, once: true, disable: 'mobile' });</script></body></html>